{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1KtpNaodcurI85HtCwAYdMj6CtodbHTaG",
      "authorship_tag": "ABX9TyNz1QLBlLVXsexMoeKm7dCg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cmdenault/cosine_similarity_analysis/blob/main/CSV_Analysis_Report_Generator_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CSV Comparison Report Generator"
      ],
      "metadata": {
        "id": "uNKrnClJGj3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cassidy Denault - Earth Economics - 2025"
      ],
      "metadata": {
        "id": "GHw9Ipii3aEP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compares two csv's: one with info gathered from analysts (expected, benchmark, ground truth) and one with info gathered from our AI powered application (generated) for the *identified important fields*\n",
        "\n",
        "Uses pretrained BERT model w/ cosine similarity.\n",
        "\n",
        "Creates a downloadable csv containing contents of both for easy comparison + similarity score evaluations"
      ],
      "metadata": {
        "id": "KE3hDtBnhVn-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO:\n",
        "\n",
        "\n",
        "*   Add more fields (landcover, etc)\n",
        "*   Computer score by section\n",
        "*   For select fields, if it doesn't match, it is a 0 (?)\n",
        "\n"
      ],
      "metadata": {
        "id": "tZNGVN-OUgFK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## User Guide"
      ],
      "metadata": {
        "id": "-70NucBmvCMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To conduct an analysis for 2 research papers. Open the analysis [data table](https://docs.google.com/spreadsheets/d/1jCMh0DtNmXQ8VLkwhWsx8AMREu6xhxVOgg50rKbGglQ/edit?usp=sharing) & follow the steps below."
      ],
      "metadata": {
        "id": "cBXcRrAWyGYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Run the **import dependencies** cells\n",
        "2.   Generate AI model xlsx file. You'll need to **export the result table as a csv.** **Record the estimated run time in seconds.**\n",
        "3.   Get expected file csv. Should be ONLY column name & value\n",
        "4.   **Upload** both in files tab to the left\n",
        "5.   Right click to get their file path and put in correct read_csv function\n",
        "6.   To run program and get downloadable **select the Upload Data Cell**\n",
        "7. Go to **Runtime tab above -> Run Cell And Below**.\n",
        "8. Result is downloaded\n",
        "9. Add the scores + other data to the analysis table [here](https://docs.google.com/spreadsheets/d/1jCMh0DtNmXQ8VLkwhWsx8AMREu6xhxVOgg50rKbGglQ/edit?usp=sharing)\n"
      ],
      "metadata": {
        "id": "fM9HiwIAvFQi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import Dependencies"
      ],
      "metadata": {
        "id": "Bh1pdVz3yA-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive # allow retrieval of files from your drive\n",
        "from google.colab import files # for downloading result\n",
        "import pandas as pd\n",
        "\n",
        "# cosine similarity models\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "Par8z68W-VOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "00QFi7uF5FB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload Data"
      ],
      "metadata": {
        "id": "5DDsDAo8oI8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload data to the files tab (it will only exist there for local runtime). You can right click to get path and insert to the read_csv function."
      ],
      "metadata": {
        "id": "PYJ1gZHdpUOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload TRAINING DATA CSV\n",
        "expected = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/colab_data/new_expected.csv')\n",
        "\n",
        "# Upload AI GENERATED CSV\n",
        "derived = pd.read_csv('/content/Earth Economics Analysis - derived.csv')\n",
        "derived"
      ],
      "metadata": {
        "id": "E5Cs7V6Hvzh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean Data"
      ],
      "metadata": {
        "id": "IOI9kPKjoRIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "derived\n"
      ],
      "metadata": {
        "id": "kIW4mu15jhOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "expected"
      ],
      "metadata": {
        "id": "jNT4y1X0jlXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to Extract the Bibliographic Journal Info from Training data"
      ],
      "metadata": {
        "id": "0QPSwF0vYaBI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And add the individual fields to the expected_dict"
      ],
      "metadata": {
        "id": "RrGg8apcVmlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def parse_reference(expected_dict):\n",
        "    \"\"\"\n",
        "    Parse the 'Full Reference' entry in the expected dictionary to extract bibliographic details.\n",
        "\n",
        "    Args:\n",
        "        expected_dict: Dictionary containing a 'Full Reference' key\n",
        "\n",
        "    Returns:\n",
        "        Updated dictionary with extracted bibliographic details\n",
        "    \"\"\"\n",
        "    if 'Full Reference' not in expected_dict:\n",
        "        print(\"No 'Full Reference' key found in the dictionary\")\n",
        "        return expected_dict\n",
        "\n",
        "    reference = expected_dict['Full Reference'][0]\n",
        "\n",
        "    # Extract authors (everything before the year)\n",
        "    year_match = re.search(r'(\\d{4})', reference)\n",
        "    if year_match:\n",
        "        year_pos = year_match.start()\n",
        "        authors = reference[:year_pos].strip()\n",
        "        # Remove the trailing period if present\n",
        "        if authors.endswith('.'):\n",
        "            authors = authors[:-1]\n",
        "        expected_dict['Authors'] = {0: authors}\n",
        "\n",
        "        # Extract publication year\n",
        "        year = year_match.group(1)\n",
        "        expected_dict['Publication Year'] = {0: year}\n",
        "\n",
        "        # Extract title (between year and journal title)\n",
        "        # Find position after year and period\n",
        "        after_year_pos = reference.find('.', year_pos) + 1\n",
        "\n",
        "        # Find the journal pattern\n",
        "        journal_match = re.search(r'\\.([^\\.]+?)\\s+(\\d+):', reference)\n",
        "\n",
        "        if journal_match:\n",
        "            journal_start_pos = journal_match.start() + 1  # +1 to skip the period\n",
        "            title = reference[after_year_pos:journal_start_pos].strip()\n",
        "            expected_dict['Title'] = {0: title}\n",
        "\n",
        "            # Extract journal title\n",
        "            journal_title = journal_match.group(1).strip()\n",
        "            expected_dict['Journal Title'] = {0: journal_title}\n",
        "\n",
        "            # Extract issue\n",
        "            issue = journal_match.group(2)\n",
        "            expected_dict['Journal Issue'] = {0: issue}\n",
        "\n",
        "            # Extract page numbers\n",
        "            pages_match = re.search(r'(\\d+)-(\\d+)', reference)\n",
        "            if pages_match:\n",
        "                start_page = pages_match.group(1)\n",
        "                end_page = pages_match.group(2)\n",
        "                expected_dict['Journal Start Page'] = {0: start_page}\n",
        "                expected_dict['Journal End Page'] = {0: end_page}\n",
        "\n",
        "    return expected_dict\n",
        "\n",
        "# Test with the sample data\n",
        "# sample_dict = {\n",
        "#     'Full Reference': {0: 'Aanesen, M., Armstrong, C. W., Czajkowski, M., Falk-Petersen, J., Hanley, N., Navrud, S. 2015. Willingness to pay for unfamiliar public goods: Preserving cold-water coral in Norway. Ecological Economics 112: 53-67.'}\n",
        "# }\n",
        "\n",
        "# result = parse_reference(sample_dict)\n",
        "# print(\"Updated dictionary:\")\n",
        "# for key, value in result.items():\n",
        "#     print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "PzpXgv65gPmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dictionary the CSVs"
      ],
      "metadata": {
        "id": "WRtjeqIIQ06d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected dictionary (training data) in format:\n",
        "\n",
        "*   Expected dictionary (training data) in format:\n",
        "\n",
        "    ***{column name} : {value in a list}***\n",
        "\n",
        "    Access: expected_dict['Author(s) (Primary)'][0]\n",
        "\n",
        "\n",
        "*   Derived Dictionary (ai gen data) in format:\n",
        "\n",
        "    ***{column name} : {value}***\n",
        "\n",
        "    Access: derived_dict['Author(s) (Primary)']\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EnppIgSlVyh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "expected_dict = expected.to_dict()\n",
        "\n",
        "\n",
        "# Make the derived csv into a dictionary with row 0 contents as the key and row 1 contents as the value\n",
        "derived_dict = derived.iloc[0:2].T.set_index(0).squeeze().to_dict()\n",
        "derived_dict\n",
        "\n",
        "# print dictionary\n",
        "# print(expected_dict['Author(s) (Primary)'][0])\n",
        "# expected_dict\n",
        "\n",
        "#print(derived_dict['Authors (semi-colon separated)'])\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pkJwfbK3Qwv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "expected_dict = parse_reference(expected_dict) # parse the individual bibliographic cols\n",
        "\n",
        "# print(derived_dict['Author(s) (Primary)'][0])\n",
        "print(expected_dict['Authors'][0])\n",
        "\n",
        "expected_value = str(expected_dict.get('Authors', {}).get(0, \"nan\"))\n",
        "print(expected_value)"
      ],
      "metadata": {
        "id": "obZy0jWjXXBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre Processing"
      ],
      "metadata": {
        "id": "8A7BQhlSoVKb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the model"
      ],
      "metadata": {
        "id": "kcY7IG91zj8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from math import nan\n",
        "\n",
        "# Initializing the Sentence Transformer model using BERT with mean-tokens pooling\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "# Load the model\n",
        "# model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "AEUlmKHxjGdR",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Detail Columns to Compare"
      ],
      "metadata": {
        "id": "MtsLoUGVzxnR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dictionary details the columns that will be compared by the report generator.\n",
        "\n",
        "\n",
        "\n",
        "We match the column name in training data csv & column name in ai generated csv. The training data has different column names than the desired ai generated template for the same field.\n",
        "\n",
        "\n",
        "\n",
        "Additionally, only the fields identified by the sponsor as important (** +) are listed."
      ],
      "metadata": {
        "id": "-epKucSnz13t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exp_to_derived = {\n",
        "    # {column name in expected csv} : {column name in derived ai generated}\n",
        "    \"Authors\": \"Authors (semi-colon separated)\",\n",
        "    # \"Full Reference\": \"\",\n",
        "    \"Journal Title\": \"Journal Title\",\n",
        "    \"Journal Issue\": \"Journal Issue\",\n",
        "    \"Journal Start Page\": \"Journal Start Page\",\n",
        "    \"Journal End Page\": \"Journal End Page\",\n",
        "    \"Title\": \"Title\",\n",
        "    \"Publication Year\": \"Year\",\n",
        "    \"Reference Type\": \"Publication Type\",\n",
        "    \"Continent\": \"Continent (semi-colon separated)\",\n",
        "    \"Country\": \"Country (semi-colon separated)\",\n",
        "    #\"\": \"Subcountry (semi-colon separated)\", # could not find this in the training data cols\n",
        "    \"Landcover\": \"Ecosystem General\", # could not find this in the training data cols col b, c, d\n",
        "    \"Ecosystem Service Category\": \"Ecosystem Service Category\",\n",
        "    \"Ecosystem Service General\": \"Ecosystem Service General\",\n",
        "    \"Valuation Methodology General\": \"Valuation Methodology General\",\n",
        "    \"Valuation Methodology Specific\": \"Valuation Methodology Specific\",\n",
        "\n",
        "    # the fields below not marked as important, but added for thoroughness\n",
        "    \"Valuation Type\": \"Calculation Type\",\n",
        "    \"Low (Published)\": \"Low\",\n",
        "    \"High (Published)\": \"High\"\n",
        "    # \"\": \"\",\n",
        "    # \"\": \"\",\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "r6IVt0CoadIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enums = { \"Reference Type\", \"Continent\", \"Landcover\", \"Ecosystem Service Category\", \"Ecosystem Service General\", \"Valuation Methodology General\", \"Valuation Methodology Specific\", \"Valuation Type\"}"
      ],
      "metadata": {
        "id": "Vs_-tr3agvAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Report Generator Code"
      ],
      "metadata": {
        "id": "exDKaKEg1iEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to compare cell & generate a similarity score\n",
        "\n",
        "Input:\n",
        "*   expected accepted dictionary\n",
        "*   ai derived dictionary\n",
        "*   column name matching dictionary\n",
        "*   model you wish to use to generate similarity scores\n",
        "\n",
        "\n",
        "Returns:\n",
        "*   report containing contents and scores in form of a data frame\n"
      ],
      "metadata": {
        "id": "ew0CDZ5KqGnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def generate_comparison_csv(expected_dict, derived_dict, exp_to_derived, model):\n",
        "    \"\"\"\n",
        "    Generate a CSV comparing values between expected_dict and derived_dict based on mapping in exp_to_derived.\n",
        "\n",
        "    Args:\n",
        "        expected_dict: Dictionary with expected values\n",
        "        derived_dict: Dictionary with derived values\n",
        "        exp_to_derived: Mapping from expected_dict keys to derived_dict keys\n",
        "        model: Sentence transformer model for calculating similarity\n",
        "\n",
        "    Returns:\n",
        "        DataFrame containing the comparison\n",
        "    \"\"\"\n",
        "\n",
        "    # Fields that should match or get 0\n",
        "    enums = { \"Reference Type\", \"Continent\", \"Landcover\", \"Ecosystem Service Category\", \"Ecosystem Service General\", \"Valuation Methodology General\", \"Valuation Methodology Specific\", \"Valuation Type\"}\n",
        "\n",
        "\n",
        "    bibliographic = {\"Authors\", \"Journal Title\", \"Journal Issue\", \"Journal Start Page\", \"Journal End Page\", \"Title\", \"Publication Year\", \"Reference Type\"}\n",
        "    ecosystem = {\"Continent\", \"Country\", \"Landcover\", \"Ecosystem Service Category\", \"Ecosystem Service General\", }\n",
        "    valuation = {\"Valuation Methodology General\", \"Valuation Methodology Specific\", \"Valuation Type\", \"Low (Published)\", \"High (Published)\"}\n",
        "\n",
        "\n",
        "    # Initialize lists for rows\n",
        "    expected_values = []\n",
        "    derived_values = []\n",
        "    similarity_scores = []\n",
        "\n",
        "    # Track sum for analysis\n",
        "    sum_scores = 0\n",
        "    valid_comparisons = 0\n",
        "\n",
        "    sum_bibliographic = 0\n",
        "    sum_ecosystem = 0\n",
        "    sum_valuation = 0\n",
        "\n",
        "    # Create column list\n",
        "    columns = list(exp_to_derived.keys())\n",
        "\n",
        "    # Process each column using expected column name\n",
        "    for exp_col in columns:\n",
        "        # Get corresponding derived column\n",
        "        derived_col = exp_to_derived[exp_col]\n",
        "\n",
        "        # Get values (handle missing keys gracefully)\n",
        "        expected_value = str(expected_dict.get(exp_col, {}).get(0, \"nan\"))\n",
        "        derived_value = str(derived_dict.get(derived_col, \"nan\"))\n",
        "\n",
        "        # Clean up values\n",
        "        expected_value = expected_value if expected_value != \"nan\" else \"\"\n",
        "        derived_value = derived_value if derived_value != \"nan\" else \"\"\n",
        "\n",
        "        # Add to values lists\n",
        "        expected_values.append(expected_value)\n",
        "        derived_values.append(derived_value)\n",
        "\n",
        "\n",
        "\n",
        "        score = -1.0\n",
        "\n",
        "        # Calculate similarity for fields\n",
        "        if expected_value == \"\" and derived_value == \"\":\n",
        "            # Both empty\n",
        "            score = 1.0\n",
        "            # similarity_scores.append(1.0)\n",
        "            sum_scores += 1.0\n",
        "        elif expected_value == \"\" and derived_value != \"\":\n",
        "            # No expected, derived generated something\n",
        "            score = 1.0\n",
        "            #similarity_scores.append(1.0)\n",
        "            sum_scores += 1.0\n",
        "        elif expected_value != \"\" and derived_value == \"\":\n",
        "            # Expected something, derived generated nothing\n",
        "            score = 0.0\n",
        "            #similarity_scores.append(0.0)\n",
        "            valid_comparisons += 1\n",
        "        else:\n",
        "            # Both have values, calculate similarity\n",
        "            try:\n",
        "                expected_embeddings = model.encode(expected_value)\n",
        "                derived_embeddings = model.encode(derived_value)\n",
        "\n",
        "                score = cosine_similarity([expected_embeddings], [derived_embeddings], dense_output=True)[0][0]\n",
        "\n",
        "                # Exact matches are expected for enum fields\n",
        "                if exp_col in enums and score < 0.75:\n",
        "                  score = 0.0001\n",
        "                  #similarity_scores.append(0.0001)\n",
        "\n",
        "                #else: # Non enum field, use score\n",
        "                  #score = score\n",
        "                  #similarity_scores.append(score)\n",
        "                sum_scores += score\n",
        "\n",
        "                valid_comparisons += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error calculating similarity for {exp_col}: {e}\")\n",
        "                similarity_scores.append(0.0)\n",
        "\n",
        "\n",
        "        similarity_scores.append(round(score, 2))\n",
        "\n",
        "        if exp_col in bibliographic:\n",
        "          sum_bibliographic += score\n",
        "        elif exp_col in ecosystem:\n",
        "          sum_ecosystem += score\n",
        "        elif exp_col in valuation:\n",
        "          sum_valuation += score\n",
        "\n",
        "\n",
        "    # Calculate average similarity score\n",
        "    avg_score = round(sum_scores / len(columns) if columns else 0, 2)\n",
        "\n",
        "    # Calculate average similarity score without both empty and generating when not expected\n",
        "    avg_score_sub = round((sum_scores - (len(columns) - valid_comparisons)) / valid_comparisons if valid_comparisons > 0 else 0, 2)\n",
        "\n",
        "\n",
        "    avg_ecosytem = round(sum_ecosystem / len(ecosystem) if ecosystem else 0, 2)\n",
        "    avg_valuation = round(sum_valuation / len(valuation) if valuation else 0, 2)\n",
        "    avg_bibliographic = round(sum_bibliographic / len(bibliographic) if bibliographic else 0, 2)\n",
        "\n",
        "\n",
        "\n",
        "    # Create the DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        'Column': columns,\n",
        "        'Expected': expected_values,\n",
        "        'Derived': derived_values,\n",
        "        'Similarity': similarity_scores\n",
        "    })\n",
        "\n",
        "    # Add a row for average similarity\n",
        "    avg_row = pd.DataFrame({\n",
        "        'Column': ['Average Similarity'],\n",
        "        'Expected': [''],\n",
        "        'Derived': [''],\n",
        "        'Similarity': [avg_score]\n",
        "    })\n",
        "\n",
        "    # Add a row for subset average similarity\n",
        "    avg_row_sub = pd.DataFrame({\n",
        "        'Column': ['Average Similarity of Filled Expected Fields', 'Bibliographic Fields Average Similarity', 'Ecosytem Desc Fields Average Similarity', 'Valuation Fields Average Similarity'],\n",
        "        'Expected': ['', '', '', ''],\n",
        "        'Derived': ['', '', '', ''],\n",
        "        'Similarity': [avg_score_sub, avg_bibliographic, avg_ecosytem, avg_valuation]\n",
        "    })\n",
        "\n",
        "    result_df = pd.concat([df, avg_row, avg_row_sub], ignore_index=True)\n",
        "    #result_df = pd.concat([df, avg_row_sub], ignore_index=True)\n",
        "\n",
        "    return result_df\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "# Sample dictionaries\n",
        "# expected_dict = {\n",
        "#     'Authors': {0: 'Aanesen, M., Armstrong, C. W., Czajkowski, M., Falk-Petersen, J., Hanley, N., Navrud, S.'},\n",
        "#     'Journal Title': {0: 'Ecological Economics'},\n",
        "#     'Journal Issue': {0: '112'},\n",
        "#     'Journal Start Page': {0: '53'},\n",
        "#     'Journal End Page': {0: '67'},\n",
        "#     'Title': {0: 'Willingness to pay for unfamiliar public goods: Preserving cold-water coral in Norway'},\n",
        "#     'Publication Year': {0: '2015'},\n",
        "#     'Reference Type': {0: 'Journal Article'},\n",
        "#     'Continent': {0: 'Europe'},\n",
        "#     'Country': {0: 'Norway'},\n",
        "#     'Ecosystem Service Category': {0: 'Cultural'},\n",
        "#     'Ecosystem Service General': {0: 'Recreation'},\n",
        "#     'Valuation Methodology General': {0: 'Stated Preference'},\n",
        "#     'Valuation Methodology Specific': {0: 'Choice Experiment'},\n",
        "#     'Valuation Type': {0: 'Primary'}\n",
        "# }\n",
        "\n",
        "# derived_dict = {\n",
        "#     'Authors (semi-colon separated)': 'Aanesen, M.; Armstrong, C. W.; Czajkowski, M.; Falk-Petersen, J.; Hanley, N.; Navrud, S.',\n",
        "#     'Journal Title': 'Ecological Economics',\n",
        "#     'Journal Issue': '112',\n",
        "#     'Journal Start Page': '53',\n",
        "#     'Journal End Page': '67',\n",
        "#     'Title': 'Willingness to pay for unfamiliar public goods: Preserving cold-water coral in Norway',\n",
        "#     'Year': '2015',\n",
        "#     'Publication Type': 'Journal Article',\n",
        "#     'Continent (semi-colon separated)': 'Europe',\n",
        "#     'Country (semi-colon separated)': 'Norway',\n",
        "#     'Ecosystem Service Category': 'Provisioning',\n",
        "#     'Ecosystem Service General': 'Recreation',\n",
        "#     'Valuation Methodology General': 'Stated Preference',\n",
        "#     'Valuation Methodology Specific': 'Choice Experiment',\n",
        "#     'Calculation Type': 'Primary'\n",
        "# }\n",
        "\n",
        "# exp_to_derived = {\n",
        "#     \"Authors\": \"Authors (semi-colon separated)\",\n",
        "#     \"Journal Title\": \"Journal Title\",\n",
        "#     \"Journal Issue\": \"Journal Issue\",\n",
        "#     \"Journal Start Page\": \"Journal Start Page\",\n",
        "#     \"Journal End Page\": \"Journal End Page\",\n",
        "#     \"Title\": \"Title\",\n",
        "#     \"Publication Year\": \"Year\",\n",
        "#     \"Reference Type\": \"Publication Type\",\n",
        "#     \"Continent\": \"Continent (semi-colon separated)\",\n",
        "#     \"Country\": \"Country (semi-colon separated)\",\n",
        "#     \"Ecosystem Service Category\": \"Ecosystem Service Category\",\n",
        "#     \"Ecosystem Service General\": \"Ecosystem Service General\",\n",
        "#     \"Valuation Methodology General\": \"Valuation Methodology General\",\n",
        "#     \"Valuation Methodology Specific\": \"Valuation Methodology Specific\",\n",
        "#     \"Valuation Type\": \"Calculation Type\"\n",
        "# }\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ITthoB5DxQug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Analysis"
      ],
      "metadata": {
        "id": "c2U-_-yn7ngS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the comparison\n",
        "result = generate_comparison_csv(expected_dict, derived_dict, exp_to_derived, model)\n"
      ],
      "metadata": {
        "id": "dJf3tSIrqjfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Report"
      ],
      "metadata": {
        "id": "3b2UaydsoaSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to CSV\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "# Generate the filename with the current date\n",
        "now = datetime.now()\n",
        "filename = f\"comparison_report_{now.month}_{now.day}_{now.year}.csv\"\n",
        "    # example: comparison_results_5_13_2025.csv for date 5/13/2025\n",
        "\n",
        "# Save the DataFrame to the generated filename\n",
        "result.to_csv(filename, index=False)\n",
        "\n",
        "# Print the filename\n",
        "#print(f\"Comparison results saved to: {filename}\")\n"
      ],
      "metadata": {
        "id": "iKsvFHA5159Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(filename)"
      ],
      "metadata": {
        "id": "45j-kPHexzCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sources & Research\n",
        "\n",
        "\n",
        "*   [Used this techinque heavily](https://medium.com/@ahmedmellit/text-similarity-implementation-using-bert-embedding-in-python-1efdb5194e65)\n",
        "*   [Text similarity basics, cosine especially](https://www.newscatcherapi.com/blog/ultimate-guide-to-text-similarity-with-python#toc-4)\n",
        "*   cosine similarity via sklearn\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9e4NcrH8CIUz"
      }
    }
  ]
}